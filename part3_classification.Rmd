---
title: "Final Project Part 3: Classification"
author: "Nick Tedesco"
date: "2022-12-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Package and Data Loading

```{r packages}

library(ggplot2)
library(tidyverse)
library(caret)
library(pROC)

```

```{r data}

data <- read.csv('/Users/nick/Documents/GSPH/INFSCI 2595/fall2022_finalproject.csv')

head(data)

```

```{r derived features}

data <- data %>% 
  mutate(
    x5 = 1 - (x1 + x2 + x3 + x4), 
    w = x2 / (x3 + x4), 
    z = (x1 + x2) / (x4 + x5), 
    t = v1 * v2, 
    y = boot::logit(output), 
    outcome = ifelse(output < 0.33, 'event', 'non_event'),
    outcome = factor(outcome, levels = c("event", "non_event"))
  )

head(data)

```

## Part 3: Classification

In this section, we will again evaluate our models using test set performance (accuracy, AUC). We will use the same train/test split that was used in the Regression section. Let's start by generating our data.

```{r classification train test split}

## set seed for reproducibility
set.seed(15213)

## generate samples
sub_sample <- sample(nrow(data), size = nrow(data)*0.80)

train <- data[sub_sample, ]
test <- data[-sub_sample, ]

## subset to base set, 
class_base_train <- train %>% select(x1:m, outcome) 
class_base_test <- test %>% select(x1:m, outcome) 

## subset to expanded set, 
class_expanded_train <- train %>% select(x1:m, x5:t, outcome) 
class_expanded_test <- test %>% select(x1:m, x5:t, outcome) 

```

Now, let's fit the models. First, fit the three models using the base feature set. 

```{r Classification Base Feature Set Models}

## all linear additive features
class_baseMod1 <- glm(formula = outcome ~ ., family = "binomial", data = class_base_train)

## interaction of the categorical input with all continuous inputs
class_baseMod2 <- glm(formula = outcome ~ m * ., family = "binomial", data = class_base_train)

## all pair-wise interactions of the continuous inputs
class_baseMod3 <- glm(formula = outcome ~ .^2, family = "binomial", data = (class_base_train %>% select(-m)))

```

Next, fit the three models using the expanded feature set. 

```{r Classification Expanded Feature Set Models}

## linear additive features
class_expandedMod1 <- glm(formula = outcome ~ ., family = "binomial", data = class_expanded_train)

## interaction of the categorical input with continuous features
class_expandedMod2 <- glm(formula = outcome ~ m * ., family = "binomial", data = class_expanded_train)

## pair-wise interactions between the continuous features
class_expandedMod3 <- glm(formula = outcome ~ .^2, family = "binomial", data = (class_expanded_train %>% select(-m)))

```

Finally, fit the three models using linear basis functions.

```{r Classification Linear Basis Function Models}

class_basisMod1 <- glm(formula = outcome ~ m + (x1 + I(x1^2)) * (x2 + x3 + x4 + z + I(z^2)) + 
                                 v1 + v2 + v3 + v4 + v5 + w + I(w^2), 
                       family = "binomial", data = class_expanded_train)

class_basisMod2 <- glm(formula = outcome ~ m + (x1 + I(x1^2) + I(x1^3)) * (x2 + x3 + x4 + z + I(z^2)) + 
                                 w + v1 + v2 + v3 + v4 + v5, 
                       family = "binomial", data = class_expanded_train)

class_basisMod3 <- glm(formula = outcome ~ m + (x1 + sin(x1) + cos(x1)) * (x2 + x3 + x4 + z + I(z^2) + w) + 
                                 (v1 + v2 + v3 + v4 + v5), 
                       family = "binomial", data = class_expanded_train)

```

I received the following warning after fitting the three basis models: Warning: glm.fit: fitted probabilities numerically 0 or 1 occurredWarning: glm.fit: fitted probabilities numerically 0 or 1 occurredWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred. 

This simply means that some of the observations in our models have fit (predicted probabilities) of almost exactly 0 or 1. After doing some Googling, it looks like we can simply ignore this warning and proceed. 

```{r Classification Model Test Set AUC}

class_base_pred1 <- predict(class_baseMod1, newdata = class_base_test, type = 'response')
class_base_auc1 <- auc(class_base_test$outcome, as.numeric(class_base_pred1))

class_base_pred2 <- predict(class_baseMod2, newdata = class_base_test, type = 'response')
class_base_auc2 <- auc(class_base_test$outcome, as.numeric(class_base_pred2))

class_base_pred3 <- predict(class_baseMod3, newdata = class_base_test, type = 'response')
class_base_auc3 <- auc(class_base_test$outcome, as.numeric(class_base_pred3))

class_expanded_pred1 <- predict(class_expandedMod1, newdata = class_expanded_test, type = 'response')
class_expanded_auc1 <- auc(class_expanded_test$outcome, as.numeric(class_expanded_pred1))

class_expanded_pred2 <- predict(class_expandedMod2, newdata = class_expanded_test, type = 'response')
class_expanded_auc2 <- auc(class_expanded_test$outcome, as.numeric(class_expanded_pred2))

class_expanded_pred3 <- predict(class_expandedMod3, newdata = class_expanded_test, type = 'response')
class_expanded_auc3 <- auc(class_expanded_test$outcome, as.numeric(class_expanded_pred3))

class_basis_pred1 <- predict(class_basisMod1, newdata = class_expanded_test, type = 'response')
class_basis_auc1 <- auc(class_expanded_test$outcome, as.numeric(class_basis_pred1))

class_basis_pred2 <- predict(class_basisMod2, newdata = class_expanded_test, type = 'response')
class_basis_auc2 <- auc(class_expanded_test$outcome, as.numeric(class_basis_pred2))

class_basis_pred3 <- predict(class_basisMod3, newdata = class_expanded_test, type = 'response')
class_basis_auc3 <- auc(class_expanded_test$outcome, as.numeric(class_basis_pred3))

class_auc_df <- data.frame(model = c("class_baseMod1", "class_baseMod2", "class_baseMod3", 
                                     "class_expandedMod1", "class_expandedMod2", "class_expandedMod3", 
                                     "class_basisMod1", "class_basisMod2", "class_basisMod3"),
                           auc = c(class_base_auc1, class_base_auc2, class_base_auc3, 
                                   class_expanded_auc1, class_expanded_auc2, class_expanded_auc3, 
                                   class_basis_auc1, class_basis_auc2, class_basis_auc3))

class_auc_df %>% ggplot() + 
  geom_point(aes(x = as.factor(model), y = auc)) + 
  xlab("Model") + 
  ylab("AUC") + 
  ggtitle("Test Set AUC for each of the Nine Logistic Regression Models") + 
  theme_bw() +
  theme(axis.text.x = element_text(face = "bold", angle = 90))

```

Similar to the regression section, the three basis models resulted in the best test set performance (in terms of AUC). class_basisMod3 was the best at predicting the outcome. 

Now, lets take a look at the coefficient summary for these three models. 

```{r class coef summary top 3: class_basisMod3}

summary(class_basisMod3)

```

```{r class coef summary top 3: class_basisMod2}

summary(class_basisMod2)

```

```{r class coef summary top 3: class_basisMod1}

summary(class_basisMod1)

```

Since so many of the features are insignificant across the three models, it is relatively difficult to determine which features are important in predicting the binary outcome. However, we see that x1, w, v1, and v3 have significant features in two of the three models. According to class_basisMod3, z also seems to be an important input. class_basisMod3 also values the interactions between I(x1^2) and many of the other x variables (x2, x3), as well as z and I(z^2).

### iiiB) Bayesian Logistic Regression Models

First, we must define our functions for using Bayesian Logistic Regression with Laplace Approximation.

```{r Bayesian Functions: logistic_logpost}

logistic_logpost <- function(unknowns, my_info)
{
  # extract the design matrix and assign to X
  X <- my_info$design_matrix
  
  # calculate the linear predictor
  eta <- as.vector(X %*% as.matrix(unknowns))
  
  # calculate the event probability
  mu <- boot::inv.logit(eta)
  
  # evaluate the log-likelihood
  log_lik <- sum(dbinom(x = my_info$yobs, 
                        size = 1,
                        prob = mu,
                        log = TRUE))
  
  # evaluate the log-prior
  log_prior <- sum(dnorm(x = unknowns,
                         mean = my_info$mu_beta,
                         sd = my_info$tau_beta,
                         log = TRUE))
  
  # sum together
  log_lik + log_prior
  
}

```

```{r Bayesian Functions: my_laplace}

my_laplace <- function(start_guess, logpost_func, ...)
{
  # code adapted from the `LearnBayes`` function `laplace()`
  fit <- optim(start_guess,
               logpost_func,
               gr = NULL,
               ...,
               method = "BFGS",
               hessian = TRUE,
               control = list(fnscale = -1, maxit = 1001))
  
  mode <- fit$par
  post_var_matrix <- -solve(fit$hessian)
  p <- length(mode)
  int <- p/2 * log(2 * pi) + 0.5 * log(det(post_var_matrix)) + logpost_func(mode, ...)
  # package all of the results into a list
  list(mode = mode,
       var_matrix = post_var_matrix,
       log_evidence = int,
       converge = ifelse(fit$convergence == 0,
                         "YES", 
                         "NO"),
       iter_counts = as.numeric(fit$counts[1]))
}

```

```{r Bayesian Functions: generate_glm_post_samples}

generate_glm_post_samples <- function(mvn_result, num_samples)
{
  # specify the number of unknown beta parameters
  length_beta <- length(mvn_result$mode)
  
  # generate the random samples
  beta_samples <- MASS::mvrnorm(n = num_samples, 
                                mu = mvn_result$mode,
                                Sigma = mvn_result$var_matrix)
  
  # change the data type and name
  beta_samples %>% 
    as.data.frame() %>% tibble::as_tibble() %>% 
    purrr::set_names(sprintf("beta_%02d", (1:length_beta) - 1))
}

```

```{r Bayesian Functions: post_logistic_pred_samples}

post_logistic_pred_samples <- function(Xnew, Bmat)
{
  # calculate the linear predictor at all prediction points and posterior samples
  eta_mat <- Xnew %*% t(Bmat)
  
  # calculate the event probability
  mu_mat <- boot::inv.logit(eta_mat)
  
  # book keeping
  list(eta_mat = eta_mat, mu_mat = mu_mat)
}

```

```{r Bayesian Functions: summarize_logistic_pred_from_laplace}

summarize_logistic_pred_from_laplace <- function(mvn_result, Xtest, num_samples)
{
  # generate posterior samples of the beta parameters
  betas <- generate_glm_post_samples(mvn_result, num_samples)
  
  # data type conversion
  betas <- as.matrix(betas)
  
  # make posterior predictions on the test set
  pred_test <- post_logistic_pred_samples(Xtest, betas)
  
  # calculate summary statistics on the posterior predicted probability
  # summarize over the posterior samples
  
  # posterior mean, should you summarize along rows (rowMeans) or 
  # summarize down columns (colMeans) ???
  mu_avg <- rowMeans(pred_test$mu_mat)
  
  # posterior quantiles
  mu_q05 <- apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.05)
  mu_q95 <- apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.95)
  
  # book keeping
  tibble::tibble(
    mu_avg = mu_avg,
    mu_q05 = mu_q05,
    mu_q95 = mu_q95
  ) %>% 
    tibble::rowid_to_column("pred_id")
}

```

For the next part part, I will refit the best model from iA (class_basisMod3). I will also refit class_expandedMod3 - I chose this model because it seems like the interactions between continuous features (especially when including those in the expanded feature set) are important for predicting the outcome. This is evident from both test set AUC and the model summaries. 

(In other words, I'm fitting the same two models from the regression part, but their classification counterparts! It seems like the classification section is going very similarly to the regression section).

First, we must make the outcome binary to comply with our functions.

```{r}

binary.outcome <- case_when(class_expanded_train$outcome == "event" ~ 1, TRUE ~ 0)

```

Now, we can fit the models

```{r class_bayesMod1 prep}

class_bayes_X01 <- model.matrix(outcome ~ m + (x1 + sin(x1) + cos(x1)) * (x2 + x3 + x4 + z + I(z^2) + w) + 
                               (v1 + v2 + v3 + v4 + v5), data = class_expanded_train)

class_bayes_info01 <- list(
  yobs = binary.outcome,
  design_matrix = class_bayes_X01,
  mu_beta = 0,
  tau_beta = 5
)

```

```{r class_bayesMod1}

class_bayes_laplace01 <- my_laplace(rep(0, ncol(class_bayes_X01)), logistic_logpost, class_bayes_info01)
class_bayes_laplace01 %>% glimpse()

```

```{r class_bayesMod2 prep}

class_bayes_X02 <- model.matrix(outcome ~ .^2, data = (class_expanded_train %>% select(-m)))

class_bayes_info02 <- list(
  yobs = binary.outcome,
  design_matrix = class_bayes_X02,
  mu_beta = 0,
  tau_beta = 5
)

```

```{r reg_bayesMod2}

class_bayes_laplace02 <- my_laplace(rep(0, ncol(class_bayes_X02)), logistic_logpost, class_bayes_info02)
class_bayes_laplace02 %>% glimpse()

```

Once again, we will use test set AUC to evaluate the models. First, let's generate our posterior predictions.

```{r class_bayesMod1 posterior predictions}

class_bayes_testX01 <- model.matrix(outcome ~ m + (x1 + sin(x1) + cos(x1)) * (x2 + x3 + x4 + z + I(z^2) + w) + 
                      (v1 + v2 + v3 + v4 + v5), data = class_expanded_test)

class_bayes_pred01 <- summarize_logistic_pred_from_laplace(class_bayes_laplace01, class_bayes_testX01, 5000)
class_bayes_pred01 %>% glimpse()

```

```{r class_bayesMod2 posterior predictions}

class_bayes_testX02 <- model.matrix(outcome ~ .^2, data = (class_expanded_test %>% select(-m)))

class_bayes_pred02 <- summarize_logistic_pred_from_laplace(class_bayes_laplace02, class_bayes_testX02, 5000)
class_bayes_pred02 %>% glimpse()

```

Now, calculate AUC by comparing mu_avg to the actual outcome.

```{r Bayesian Logistic Model AUC}

class_bayes_auc1 <- auc(class_expanded_test$outcome, class_bayes_pred01$mu_avg)

class_bayes_auc2 <- auc(class_expanded_test$outcome, class_bayes_pred02$mu_avg)

class_bayes_auc_df <- data.frame(model = c("class_bayesMod1", "class_bayesMod2"),
                          auc = c(class_bayes_auc1, class_bayes_auc2))

class_bayes_auc_df %>% ggplot() + 
  geom_point(aes(x = as.factor(model), y = auc)) + 
  xlab("Model") + 
  ylab("AUC") + 
  ggtitle("Test Set AUC for each of the Two Bayesian Regression Models") + 
  theme_bw() +
  theme(axis.text.x = element_text(face = "bold", angle = 90))

```

The best of the two models was identified as class_bayesMod2, as according to test set AUC Next, let's show the regression coefficient posterior summary statistics for our best model. 

```{r class_bayes_summary function}

class_bayes_summary <- function(laplace_object, design_matrix){
  
  names <- c(colnames(design_matrix))
  modes <- laplace_object$mode
  sd <- sqrt(diag(laplace_object$var_matrix))
  
  df <- data.frame(Parameter = names, Estimate = modes, sdev = sd)
  
  df
    
}

```

```{r class_bayes_laplace02 summary}

class_bayes_summary02 <- class_bayes_summary(class_bayes_laplace02, class_bayes_X02)
class_bayes_summary02

```

The standard deviations for the model coefficients are quite high compared to their corresponding coefficient estimates. This is consistent with the multicolinearity issues we discussed earlier, as well as the fact that logistic regression / classification tends to produce more uncertain coefficients. 

It is worth mentioning that reg_bayesMod1 was found to be better than reg_bayesMod2 in the regression section. However, here we found that class_bayesMod1 is worse than class_bayesMod2. Perhaps this is because of our prior specifications - before, the prior wasn't as big of a deal since the regression coefficients weren't very large in magnitude. However, since the regression coefficients appear to be much larger in the classification section, perhaps the prior had more of an impact on our final Bayesian coefficients (as compared to the corresponding parts of the regression section).

### iiC) GLM Predictions

Once again, I will use the linear (as opposed to Bayesian) models for making predictions

Start by defining the prediction grid. We will do this in the same way as the regression case. 

```{r class_pred_viz_grid}

class_pred_viz_grid <- expand.grid(x1 = seq(min(data$x1), max(data$x1), length.out = 101),
                                 z = seq(min(data$z), max(data$z), length.out = 6),
                                 x2 = mean(data$x2),
                                 x3 = mean(data$x3),
                                 x4 = mean(data$x4),
                                 x5 = mean(data$x5),
                                 v1 = mean(data$v1),
                                 v2 = mean(data$v2),
                                 v3 = mean(data$v3),
                                 v4 = mean(data$v4),
                                 v5 = mean(data$v5),
                                 w = mean(data$w),
                                 t = mean(data$t),
                                 m = c("C", "D"), 
                                 KEEP.OUT.ATTRS = FALSE,
                                 stringsAsFactors = FALSE) %>% 
  as.data.frame() %>% tibble::as_tibble()

```

Now we will make the predictions and retrieve the confidence intervals. Since the predict function cannot calculate confidence intervals for logistic regression models, we must compute these ourselves. We will do this by retrieving the standard error and mean predictions (not transformed to the scale of probability). 

```{r class_linear_pred01_df}

class_linear_pred01 <- predict(class_basisMod3, newdata = class_pred_viz_grid, type = 'response')

class_linear_untransformed_pred01 <- predict(class_basisMod3, newdata = class_pred_viz_grid, se.fit = TRUE)

class01_upr_untransformed <- class_linear_untransformed_pred01$fit + (1.96 * class_linear_untransformed_pred01$se.fit)
class01_lwr_untransformed <- class_linear_untransformed_pred01$fit - (1.96 * class_linear_untransformed_pred01$se.fit)

class01_upr <- boot::inv.logit(class01_upr_untransformed)
class01_lwr <- boot::inv.logit(class01_lwr_untransformed)

class_linear_pred01_df <- cbind(class_pred_viz_grid, class_linear_pred01, class01_upr, class01_lwr)

```

```{r class_linear_pred02_df}

class_linear_pred02 <- predict(class_expandedMod3, newdata = class_pred_viz_grid, type = 'response')

class_linear_untransformed_pred02 <- predict(class_expandedMod3, newdata = class_pred_viz_grid, se.fit = TRUE)

class02_upr_untransformed <- class_linear_untransformed_pred02$fit + (1.96 * class_linear_untransformed_pred02$se.fit)
class02_lwr_untransformed <- class_linear_untransformed_pred02$fit - (1.96 * class_linear_untransformed_pred02$se.fit)

class02_upr <- boot::inv.logit(class02_upr_untransformed)
class02_lwr <- boot::inv.logit(class02_lwr_untransformed)

class_linear_pred02_df <- cbind(class_pred_viz_grid, class_linear_pred02, class02_upr, class02_lwr)

```

Finally, visualize the results.

```{r class_linear_pred01 viz}

class_linear_pred01_df %>% ggplot(aes(x = x1)) + 
  geom_ribbon(aes(ymin = class01_lwr, ymax = class01_upr), fill = 'grey') + 
  geom_line(aes(y = class_linear_pred01)) + 
  facet_wrap(facets = ~z) + 
  coord_cartesian(ylim = c(0, 1))

```

```{r class_linear_pred02 viz}

class_linear_pred02_df %>% ggplot(aes(x = x1)) + 
  geom_ribbon(aes(ymin = class02_lwr, ymax = class02_upr), fill = 'grey') + 
  geom_line(aes(y = class_linear_pred02)) + 
  facet_wrap(facets = ~z) + 
  coord_cartesian(ylim = c(0, 1))

```

Once again, the predictive trends are nowhere near the same across the two models. This makes sense, considering that we are using relatively complex basis functions in class_basisMod3 and simple linear interaction terms in class_expandedMod3. 

It is worth noting that the confidence intervals on the predictions are extreme for both models. Although this doesn't look pretty, considering the extremely large coefficient standard errors/deviations that we observed in both the linear and Bayesian models, this makes sense...

### iiiD) Train/Tune with Resampling 

First, we will prepare the caret dataset to fit our models. 

```{r make class_df_caret}

class_df_caret <- data %>% 
  select(-c(y, output))

class_df_caret %>% glimpse()

```

Next, let's define our metric and resampling method. We will have two sets of evaluation metrics, and therefore train two sets of models: one for accuracy, and one using AUC. In both cases, we will perform 10-fold 5-repeat cross validation. 

```{r caret trainControl and metric}

my_class_ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5,
                        returnData = FALSE, classProbs = TRUE,
                        summaryFunction = twoClassSummary, verboseIter = TRUE)

my_class_metric1 <- 'ROC'

my_class_ctrl2 <- trainControl(method = "repeatedcv", number = 10, repeats = 5,
                        returnData = FALSE, classProbs = TRUE, verboseIter = TRUE)

my_class_metric2 <- 'Accuracy'

```

Now, let's start fitting models. We will start by fitting the four linear models. The seed will be set in each chunk to ensure reproducibility.

```{r class caret glm additive base features}

set.seed(1234)

class_caret_lm_base_roc <- train(outcome ~ ., 
                             data = (class_df_caret %>% select(-c(x5, w, z, t))),
                             method = "glm",
                             family = "binomial",
                             metric = my_class_metric1,
                             preProcess = c("center", "scale"),
                             trControl = my_class_ctrl1)

class_caret_lm_base_acc <- train(outcome ~ ., 
                             data = (class_df_caret %>% select(-c(x5, w, z, t))),
                             method = "glm",
                             family = "binomial",
                             metric = my_class_metric2,
                             preProcess = c("center", "scale"),
                             trControl = my_class_ctrl2)

```

```{r class caret glm additive expanded features, warning = FALSE}

set.seed(1234)

class_caret_lm_expanded_roc <- train(outcome ~ ., 
                                 data = class_df_caret,
                                 method = "glm",
                                 family = "binomial",
                                 metric = my_class_metric1,
                                 preProcess = c("center", "scale"),
                                 trControl = my_class_ctrl1)

class_caret_lm_expanded_acc <- train(outcome ~ ., 
                                 data = class_df_caret,
                                 method = "glm",
                                 family = "binomial",
                                 metric = my_class_metric2,
                                 preProcess = c("center", "scale"),
                                 trControl = my_class_ctrl2)
  
```

```{r class caret glm class_basisMod3}

set.seed(1234)

class_caret_lm_basisMod3_roc <- caret::train(outcome ~ m + (x1 + sin(x1) + cos(x1)) * (x2 + x3 + x4 + z + I(z^2) + w) + 
                                         (v1 + v2 + v3 + v4 + v5), 
                                         data = class_df_caret,
                                         method = "glm",
                                         family = "binomial",
                                         metric = my_class_metric1,
                                         preProcess = c("center", "scale"),
                                         trControl = my_class_ctrl1)

class_caret_lm_basisMod3_acc <- caret::train(outcome ~ m + (x1 + sin(x1) + cos(x1)) * (x2 + x3 + x4 + z + I(z^2) + w) + 
                                         (v1 + v2 + v3 + v4 + v5), 
                                         data = class_df_caret,
                                         method = "glm",
                                         family = "binomial",
                                         metric = my_class_metric2,
                                         preProcess = c("center", "scale"),
                                         trControl = my_class_ctrl2)

```

```{r class caret glm class_expandedMod3, warning = FALSE}

set.seed(1234)

class_caret_lm_expandedMod3_roc <- train(outcome ~ .^2, 
                                     data = class_df_caret,
                                     method = "glm",
                                     family = "binomial",
                                     metric = my_class_metric1,
                                     preProcess = c("center", "scale"),
                                     trControl = my_class_ctrl1)

class_caret_lm_expandedMod3_acc <- train(outcome ~ .^2, 
                                     data = class_df_caret,
                                     method = "glm",
                                     family = "binomial",
                                     metric = my_class_metric2,
                                     preProcess = c("center", "scale"),
                                     trControl = my_class_ctrl2)

```

Next, let's train the two elastic net models. 

```{r class caret enetMod1}

set.seed(1234)

class_caret_enetMod1_roc <- train(outcome ~ m * (x1 + x2 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + w + z + t)^2, 
                              data = class_df_caret,
                              method = "glmnet",
                              metric = my_class_metric1,
                              preProcess = c("center", "scale"),
                              trControl = my_class_ctrl1)

class_caret_enetMod1_acc <- train(outcome ~ m * (x1 + x2 + x3 + x4 + x5 + v1 + v2 + v3 + v4 + v5 + w + z + t)^2, 
                              data = class_df_caret,
                              method = "glmnet",
                              metric = my_class_metric2,
                              preProcess = c("center", "scale"),
                              trControl = my_class_ctrl2)

```

```{r class caret enetMod2}

set.seed(1234)

class_caret_enetMod2_roc <- caret::train(outcome ~ m + (x1 + sin(x1) + cos(x1)) * (x2 + x3 + x4 + z + I(z^2) + w) + 
                                          (v1 + v2 + v3 + v4 + v5), 
                                     data = class_df_caret,
                                     method = "glmnet",
                                     metric = my_class_metric1,
                                     preProcess = c("center", "scale"),
                                     trControl = my_class_ctrl1)

class_caret_enetMod2_acc <- caret::train(outcome ~ m + (x1 + sin(x1) + cos(x1)) * (x2 + x3 + x4 + z + I(z^2) + w) + 
                                          (v1 + v2 + v3 + v4 + v5), 
                                     data = class_df_caret,
                                     method = "glmnet",
                                     metric = my_class_metric2,
                                     preProcess = c("center", "scale"),
                                     trControl = my_class_ctrl2)

```

Now, let's train the base and expanded models using neural networks.

```{r class caret neural network base features}

set.seed(1234)

class_caret_neuralNet_base_roc <- train(outcome ~ ., 
                                    data = (class_df_caret %>% select(-c(x5, w, z, t))),
                                    method = "nnet",
                                    metric = my_class_metric1,
                                    preProcess = c("center", "scale"),
                                    trControl = my_class_ctrl1, 
                                    trace = FALSE)

class_caret_neuralNet_base_acc <- train(outcome ~ ., 
                                    data = (class_df_caret %>% select(-c(x5, w, z, t))),
                                    method = "nnet",
                                    metric = my_class_metric2,
                                    preProcess = c("center", "scale"),
                                    trControl = my_class_ctrl2, 
                                    trace = FALSE)

```

```{r class caret neural network expanded features}

set.seed(1234)

class_caret_neuralNet_expanded_roc <- train(outcome ~ ., 
                                        data = class_df_caret,
                                        method = "nnet",
                                        metric = my_class_metric1,
                                        preProcess = c("center", "scale"),
                                        trControl = my_class_ctrl1, 
                                        trace = FALSE)

class_caret_neuralNet_expanded_acc <- train(outcome ~ ., 
                                        data = class_df_caret,
                                        method = "nnet",
                                        metric = my_class_metric2,
                                        preProcess = c("center", "scale"),
                                        trControl = my_class_ctrl2, 
                                        trace = FALSE)

```

Next, let's train the base and expanded models using the random forest method.

```{r class caret random forest base features}

set.seed(1234)

class_caret_rf_base_roc <- train(outcome ~ ., 
                             data = (class_df_caret %>% select(-c(x5, w, z, t))),
                             method = "rf",
                             metric = my_class_metric1,
                             preProcess = c("center", "scale"),
                             trControl = my_class_ctrl1, 
                             trace = FALSE)

class_caret_rf_base_acc <- train(outcome ~ ., 
                             data = (class_df_caret %>% select(-c(x5, w, z, t))),
                             method = "rf",
                             metric = my_class_metric2,
                             preProcess = c("center", "scale"),
                             trControl = my_class_ctrl2, 
                             trace = FALSE)

```

```{r class caret random forest expanded features}

set.seed(1234)

class_caret_rf_expanded_roc <- train(outcome ~ ., 
                                 data = class_df_caret,
                                 method = "rf",
                                 metric = my_class_metric1,
                                 preProcess = c("center", "scale"),
                                 trControl = my_class_ctrl1, 
                                 trace = FALSE)

class_caret_rf_expanded_acc <- train(outcome ~ ., 
                                 data = class_df_caret,
                                 method = "rf",
                                 metric = my_class_metric2,
                                 preProcess = c("center", "scale"),
                                 trControl = my_class_ctrl2, 
                                 trace = FALSE)


```

Next, let's train the base and expanded models using the gradient boosted tree method.

```{r class caret gradient boosted tree base features, warning = FALSE, message = FALSE}

set.seed(1234)

class_caret_xgb_base_roc <- train(outcome ~ ., 
                              data = (class_df_caret %>% select(-c(x5, w, z, t))),
                              method = "xgbTree",
                              metric = my_class_metric1,
                              preProcess = c("center", "scale"),
                              trControl = my_class_ctrl1, 
                              trace = FALSE)

class_caret_xgb_base_acc <- train(outcome ~ ., 
                              data = (class_df_caret %>% select(-c(x5, w, z, t))),
                              method = "xgbTree",
                              metric = my_class_metric2,
                              preProcess = c("center", "scale"),
                              trControl = my_class_ctrl2, 
                              trace = FALSE)

```

```{r class caret gradient boosted tree expanded features, warning = FALSE, message = FALSE}

set.seed(1234)

class_caret_xgb_expanded_roc <- train(outcome ~ ., 
                                  data = class_df_caret,
                                  method = "xgbTree",
                                  metric = my_class_metric1,
                                  preProcess = c("center", "scale"),
                                  trControl = my_class_ctrl1, 
                                  trace = FALSE)

class_caret_xgb_expanded_acc <- train(outcome ~ ., 
                                  data = class_df_caret,
                                  method = "xgbTree",
                                  metric = my_class_metric2,
                                  preProcess = c("center", "scale"),
                                  trControl = my_class_ctrl2, 
                                  trace = FALSE)

```

Lastly, we will use two methods that were not discussed in class. First, let's fit the base and expanded additive models with Generalized Additive Model using Splines. 

```{r class caret gamSpline base features, message = FALSE}

set.seed(1234)

class_caret_gamSpline_base_roc <- train(outcome ~ ., 
                                    data = (class_df_caret %>% select(-c(x5, w, z, t))),
                                    method = "gamSpline",
                                    metric = my_class_metric1,
                                    preProcess = c("center", "scale"),
                                    trControl = my_class_ctrl1)

class_caret_gamSpline_base_acc <- train(outcome ~ ., 
                                    data = (class_df_caret %>% select(-c(x5, w, z, t))),
                                    method = "gamSpline",
                                    metric = my_class_metric2,
                                    preProcess = c("center", "scale"),
                                    trControl = my_class_ctrl2)

```

```{r class caret gamSpline expanded features, message = FALSE}

set.seed(1234)

class_caret_gamSpline_expanded_roc <- train(outcome ~ ., 
                                        data = class_df_caret,
                                        method = "gamSpline",
                                        metric = my_class_metric1,
                                        preProcess = c("center", "scale"),
                                        trControl = my_class_ctrl1)

class_caret_gamSpline_expanded_acc <- train(outcome ~ ., 
                                        data = class_df_caret,
                                        method = "gamSpline",
                                        metric = my_class_metric2,
                                        preProcess = c("center", "scale"),
                                        trControl = my_class_ctrl2)

```

Now, let's fit the base and expanded additive models using Multi-Layer Perceptron. 

```{r class caret multi layer perceptron base features}

set.seed(1234)

class_caret_mlp_base_roc <- train(outcome ~ ., 
                              data = (class_df_caret %>% select(-c(x5, w, z, t))),
                              method = "mlp",
                              metric = my_class_metric1,
                              preProcess = c("center", "scale"),
                              trControl = my_class_ctrl1)

class_caret_mlp_base_acc <- train(outcome ~ ., 
                              data = (class_df_caret %>% select(-c(x5, w, z, t))),
                              method = "mlp",
                              metric = my_class_metric2,
                              preProcess = c("center", "scale"),
                              trControl = my_class_ctrl2)

```

```{r class caret multi layer perceptron expanded features, message = FALSE}

set.seed(1234)

class_caret_mlp_expanded_roc <- train(outcome ~ ., 
                                  data = class_df_caret,
                                  method = "mlp",
                                  metric = my_class_metric1,
                                  preProcess = c("center", "scale"),
                                  trControl = my_class_ctrl1)

class_caret_mlp_expanded_acc <- train(outcome ~ ., 
                                  data = class_df_caret,
                                  method = "mlp",
                                  metric = my_class_metric2,
                                  preProcess = c("center", "scale"),
                                  trControl = my_class_ctrl2)

```

Finally, we will visualize a dotplot comparing resampled RMSE for all of the models trained using caret. 

```{r class caret models ROC}

caret_roc_compare <- resamples(list(GLM_base = class_caret_lm_base_roc,
                                    GLM_expanded = class_caret_lm_expanded_roc,
                                    GLM_basisMod3 = class_caret_lm_basisMod3_roc,
                                    GLM_expandedMod3 = class_caret_lm_expandedMod3_roc,
                                    ENET_interactions = class_caret_enetMod1_roc,
                                    ENET_basisMod3 = class_caret_enetMod2_roc,
                                    NNET_base = class_caret_neuralNet_base_roc,
                                    NNET_expanded = class_caret_neuralNet_expanded_roc, 
                                    RF_base = class_caret_rf_base_roc, 
                                    RF_expanded = class_caret_rf_expanded_roc, 
                                    XGB_base = class_caret_xgb_base_roc, 
                                    XGB_expanded = class_caret_xgb_expanded_roc, 
                                    gamSpline_base = class_caret_gamSpline_base_roc, 
                                    gamSpline_expanded = class_caret_gamSpline_expanded_roc, 
                                    MLP_base = class_caret_mlp_base_roc, 
                                    MLP_expanded = class_caret_mlp_expanded_roc))

dotplot(caret_roc_compare, metric = 'ROC')

```

```{r class caret models accuracy}

caret_acc_compare <- resamples(list(GLM_base = class_caret_lm_base_acc,
                                    GLM_expanded = class_caret_lm_expanded_acc,
                                    GLM_basisMod3 = class_caret_lm_basisMod3_acc,
                                    GLM_expandedMod3 = class_caret_lm_expandedMod3_acc,
                                    ENET_interactions = class_caret_enetMod1_acc,
                                    ENET_basisMod3 = class_caret_enetMod2_acc,
                                    NNET_base = class_caret_neuralNet_base_acc,
                                    NNET_expanded = class_caret_neuralNet_expanded_acc, 
                                    RF_base = class_caret_rf_base_acc, 
                                    RF_expanded = class_caret_rf_expanded_acc, 
                                    XGB_base = class_caret_xgb_base_acc, 
                                    XGB_expanded = class_caret_xgb_expanded_acc, 
                                    gamSpline_base = class_caret_gamSpline_base_acc, 
                                    gamSpline_expanded = class_caret_gamSpline_expanded_acc, 
                                    MLP_base = class_caret_mlp_base_acc, 
                                    MLP_expanded = class_caret_mlp_expanded_acc))

dotplot(caret_acc_compare, metric = 'Accuracy')

```

The XGB_expanded model was the best at maximizing both AUC and accuracy. The gamSpline_expanded model was second best in both cases. GLM_basisMod3 was the third best in terms of AUC, whereas RF_base was the third best for accuracy. 
