---
title: 'Final Project Part 4: Interpretation'
author: "Nick Tedesco"
date: "2022-12-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Package and Data Loading

```{r packages}

library(ggplot2)
library(tidyverse)
library(caret)
library(pROC)

```

```{r data}

data <- read.csv('/Users/nick/Documents/GSPH/INFSCI 2595/fall2022_finalproject.csv')

head(data)

```

```{r derived features}

data <- data %>% 
  mutate(
    x5 = 1 - (x1 + x2 + x3 + x4), 
    w = x2 / (x3 + x4), 
    z = (x1 + x2) / (x4 + x5), 
    t = v1 * v2, 
    y = boot::logit(output), 
    outcome = ifelse(output < 0.33, 'event', 'non_event'),
    outcome = factor(outcome, levels = c("event", "non_event"))
  )

head(data)

```

## Retraining Best Models

We have to retrain our best models from regression and classification in order to have access to them in this file. 

### Regression

First, we will prepare the caret dataset to fit our models. 

```{r make reg_df_caret}

reg_df_caret <- data %>% 
  select(-c(output, outcome))

reg_df_caret %>% glimpse()

```

Next, let's define our metric and resampling method. We will perform 10-fold 5-repeat cross validation.

```{r caret regression trainControl and metric}

my_ctrl <- trainControl(method = 'repeatedcv', number = 10, repeats = 5)

my_metric <- 'RMSE'

```

Finally, let's retrain our best regression model: reg_caret_xgb_expanded.

```{r caret gradient boosted tree expanded features, warning = FALSE, message = FALSE}

set.seed(1234)

reg_caret_xgb_expanded <- train(y ~ ., 
                                data = reg_df_caret,
                                method = "xgbTree",
                                metric = my_metric,
                                preProcess = c("center", "scale"),
                                trControl = my_ctrl, 
                                trace = FALSE)

```

### Classification

First, we will prepare the caret dataset to fit our models. 

```{r make class_df_caret}

class_df_caret <- data %>% 
  select(-c(y, output))

class_df_caret %>% glimpse()

```

Next, let's define our metric and resampling method. We will have two sets of evaluation metrics, and therefore train two sets of models: one for accuracy, and one using AUC. In both cases, we will perform 10-fold 5-repeat cross validation. 

```{r caret classification trainControl and metric}

my_class_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5,
                        returnData = FALSE, classProbs = TRUE,
                        summaryFunction = twoClassSummary, verboseIter = TRUE)

my_class_metric <- 'ROC'

```

Finally, let's retrain our classification best model: class_caret_xgb_expanded.

```{r class caret gradient boosted tree expanded features, warning = FALSE, message = FALSE}

set.seed(1234)

class_caret_xgb_expanded_roc <- train(outcome ~ ., 
                                  data = class_df_caret,
                                  method = "xgbTree",
                                  metric = my_class_metric,
                                  preProcess = c("center", "scale"),
                                  trControl = my_class_ctrl, 
                                  trace = FALSE)

```

## Part 4: Interpretation

As shown by the RMSE, accuracy, and AUC figures, the expanded versions of the models tended to perform better than their base counterparts. This would suggest that features in the expanded dataset are important for predicting our outcome.

Let's take a look at the most important variables in our best performing models for regression and classification. For regression, xgb_expanded performed the best. 

```{r varImp regression XGB_expanded}

plot(varImp(reg_caret_xgb_expanded))

```

The  best regression model suggests that x1 and z are the most important inputs. Now, let's look at the best classification model: xgb_expanded.

```{r varImp classification xgb_expanded}

plot(varImp(class_caret_xgb_expanded_roc))

```


The best classification model suggests that x1 and w are the most important inputs. 

Now, let's visualize the two most important variables for each case (regression and classification) with respect to the predicted logit-transformed response (regression) or predicted probability (classification). In other words, we will create two contour plots: one for the important inputs from regression (x1 and z) with respect to the predicted logit-transformed response, and one for the important inputs from classification (x1 and w) with respect to the predicted probability. 

First, we need to define two visualization grids to make predictions on: one for the best regression model and its corresponding inputs (x1 and z), and one for the classification model and its corresponding inputs (x1 and w). Later in this section, we are asked to see if the optimal variable combinations differ over different levels of m - therefore, we will also include all levels of m in the visualization grids. 

```{r reg_viz_grid}

reg_viz_grid <- expand.grid(x1 = seq(min(data$x1), max(data$x1), length.out=75),
                            z = seq(min(data$z), max(data$z), length.out=75),
                            m = c("A", "B", "C", "D", "E"),
                            x2 = mean(data$x2),
                            x3 = mean(data$x3),
                            x4 = mean(data$x4),
                            x5 = mean(data$x5),
                            v1 = mean(data$v1),
                            v2 = mean(data$v2),
                            v3 = mean(data$v3),
                            v4 = mean(data$v4),
                            v5 = mean(data$v5), 
                            w = mean(data$w),
                            t = mean(data$t), 
                            KEEP.OUT.ATTRS = FALSE, 
                            stringsAsFactors = FALSE) %>% 
  as.data.frame() %>% tibble::as_tibble()

```

```{r class_viz_grid}

class_viz_grid <- expand.grid(x1 = seq(min(data$x1), max(data$x1), length.out=75),
                              w = seq(min(data$w), max(data$w), length.out=75),
                              m = c("A", "B", "C", "D", "E"),
                              x2 = mean(data$x2),
                              x3 = mean(data$x3),
                              x4 = mean(data$x4),
                              x5 = mean(data$x5),
                              v1 = mean(data$v1),
                              v2 = mean(data$v2),
                              v3 = mean(data$v3),
                              v4 = mean(data$v4),
                              v5 = mean(data$v5), 
                              z = mean(data$z),
                              t = mean(data$t), 
                              KEEP.OUT.ATTRS = FALSE, 
                              stringsAsFactors = FALSE) %>% 
  as.data.frame() %>% tibble::as_tibble()

```

Now, make our predictions using the two visualization grids.

```{r reg_viz_grid_df}

reg_pred <- predict(reg_caret_xgb_expanded, newdata = reg_viz_grid)

reg_viz_grid_df <- cbind(reg_viz_grid, reg_pred)

```

```{r class_viz_grid_df}

class_pred <- predict(class_caret_xgb_expanded_roc, newdata = class_viz_grid, type = 'prob')

class_viz_grid_df <- cbind(class_viz_grid, class_pred)

```

```{r important regression inputs}

reg_viz_grid_df %>%
  ggplot(aes(x = x1, y = z)) + 
  geom_raster(aes(fill = reg_pred)) + 
  facet_wrap(~m) + 
  scale_fill_gradient2(low = 'blue', mid = 'white', high = 'red',
                       midpoint = max(reg_pred) - (max(reg_pred) - min(reg_pred)) / 2,
                       limits = c(min(reg_pred), max(reg_pred)))

```

```{r important classification inputs}

class_viz_grid_df %>%
  ggplot(aes(x = x1, y = w)) + 
  geom_raster(aes(fill = event)) + 
  facet_wrap(~m) + 
  scale_fill_gradient2(low = 'blue', mid = 'white', high = 'red',
                       midpoint = 0.5,
                       limits = c(0, 1))

```

As shown by the regression figure, low to median values of x1 (0.1 to 0.3) in conjunction with low to median values of z (0 to 3) minimize the logit-transformed response. This trend does not seem to differ very much across the different machines.

As shown by the classification figure, very low values of z1 and any value of w results in the lowest event probability. However, we can also see that relatively higher values of x1 (0.35 to 0.6) in conjunction with relatively higher values of w (0.4 to 1.0) also result in low event probability. This trend does not differ for most of the machines - however, we see a very slightly different trend for m = E, where moderately lower values of x1 (0.1 to 0.2) in conjunction with median (0.50) or high (0.90) values of w result in low event probability.

With all of this information in mind, we might shoot for x1 ~ 0.35, z ~ 2, and w ~ 0.50 as the most optimal input values for reducing surface corrosion. If we are using machine E, we might shoot for x1 ~ 0.1 to 0.2, z ~ 2, and w ~ 0.50. However, since these inputs depend on one another, it might be difficult able to achieve this sort of balance in practice. 

## Holdout Set Predictions

First, load the holdout set and initialize the dataframe to store our results

```{r holdout set load}

holdout <- read.csv('/Users/nick/Downloads/fall2022_holdout_inputs.csv')

holdout <- holdout %>% 
  mutate(
    x5 = 1 - (x1 + x2 + x3 + x4), 
    w = x2 / (x3 + x4), 
    z = (x1 + x2) / (x4 + x5), 
    t = v1 * v2
  )

```

Next, make predictions for the continuous outcome.

```{r y pred}

y = predict(reg_caret_xgb_expanded, newdata = holdout)

```

Now, for the categorical outcome.

```{r outcome pred}

probability = predict(class_caret_xgb_expanded_roc, newdata = holdout, type = "prob")
outcome <- ifelse(probability$event >= 0.5, "event", "nonevent")

```

Finally, organize the results into the final dataframe for download.

```{r holdout pred dataframe}

holdout_predictions <- data.frame(y = y, outcome = outcome, probability = probability$event) %>%
  tibble::rowid_to_column() %>% rename(id = rowid)

```

```{r write_csv}

write.csv(holdout_predictions, '/Users/nick/Documents/GSPH/INFSCI 2595/holdout_predictions.csv')

```


